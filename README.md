
# Auto Spell Correction for Nonâ€‘English Words Written in English Script

This repo contains a **productionâ€‘ready, fast spell corrector** aimed at Indic languages typed in Latin/English script
(Hinglish/Marathiâ€‘inâ€‘English, etc.). It handles noisy spellings in large files (>=10k lines).

## Quick Start

```bash
# 1) Put your dictionary and errors files in ./data
#    - dictionary: reference.txt (one word per line, optionally "word\tfreq")
#    - errors: errors.txt (one misspelled token per line)

# 2) Build the index (optional; done lazily on first run)
python correct.py --dictionary data/reference.txt --errors data/errors.txt --out data/output.csv

# Extra knobs
python correct.py --dictionary data/reference.txt --errors data/errors.txt --out data/output.csv \
  --max_edit 2 --topk 1 --threads 4
```

**Output format** (CSV with header):
```
File_Error,Corrected
Aum,Aam
ROM,Ram
RAAM,Ram
```

---

## Why this works (Design)

Realâ€‘world Romanized Indian words contain:
- **Vowel drift:** `a/aa`, `i/ee`, `u/oo`, `e/ai`, `o/au`
- **Aspiration drift:** `kh/k`, `ph/p`, `th/t`, `dh/d`, `bh/b`, `ch/c`, `sh/s`
- **Repeated letters & emphasis:** `raam` â†’ `ram`, `goooood` â†’ `good`
- **QWERTY typos & swaps** (Damerau transpositions).

We combine three ideas for **accuracy** and **speed**:

1. **Deleteâ€‘Index Search (SymSpellâ€‘style):**  
   Precompute all strings formed by deleting up to *d* characters from every dictionary entry.  
   Lookups turn into hash table probes â€” extremely fast.

2. **Weighted Damerauâ€‘Levenshtein distance:**  
   Edit cost is smaller for *expected* confusions (e.g., `aaâ†”a`, `eeâ†”i`, `ooâ†”u`, aspiration removal).

3. **Phonetic Key (Indicâ€‘aware):**  
   We normalize pairs (`khâ†’k`, `shâ†’s`, `aaâ†’a`, â€¦), collapse duplicates, and compare phonetic keys.  
   Candidates with matching keys get a scoring **bonus**.

The final score is:
```
score = w_edit * (âˆ’edit_distance) + w_phon * [phonetic_key_match] + w_freq * log(1+freq)
```
Topâ€‘K candidates are returned (default K=1).

### Complexity

- **Index build:** O(N * L * d) deletes (N words, average length L, max delete distance d).  
- **Query:** O(C * L) where C is candidate set size (usually small due to deletesâ€‘filter).

---

## Files

- `correct.py` â€“ CLI tool to correct a file using the engine.
- `spellfix/engine.py` â€“ Core engine (delete index + weighted DL + phonetic key).
- `spellfix/phonetics.py` â€“ Indicâ€‘aware phonetic normalization.
- `spellfix/utils.py` â€“ Small utilities and a fast DL distance.
- `data/reference.txt` â€“ Sample dictionary (feel free to replace with your full list).
- `data/errors.txt` â€“ Sample errors to test the pipeline.

---

## Notes

- The dictionary can include frequencies: `"word<TAB>count"`. Frequencies bias results to common words.
- The engine is pureâ€‘Python, no external deps, so it runs anywhere (including coding interviews).
- For **very large** dictionaries, set `--max_edit 1` for speed, or increase `--threads`.

Good luck in your interview! ðŸš€
